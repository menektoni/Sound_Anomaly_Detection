{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from functools import partial\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import tensorboard\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D, UpSampling1D, Reshape\n",
    "from tensorflow.keras.layers import Input,Flatten,Dense, Dropout, GlobalAveragePooling2D, AveragePooling1D\n",
    "from tensorflow.keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_list(directory):\n",
    "    '''\n",
    "    Load the audiofiles inside a directory.\n",
    "    '''\n",
    "    return os.listdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize1(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2(v):\n",
    "    return (v - np.min(v)) / (np.max(v)-np.min(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize3(v, m, var):\n",
    "    return (v - m) / var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '/home/belen_alastruey/PYTHON/Autoencoder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DIRECTORY + 'Descriptors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files_list(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "for file in files:\n",
    "    if \"fan\" in file:\n",
    "        descr = np.load(train_path + file)\n",
    "        descriptors.append(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr = np.asarray(descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized=[]\n",
    "m = np.mean(descr)\n",
    "var=np.std(descr)\n",
    "for v in descriptors:\n",
    "    normalized.append(normalize2(v))\n",
    "    #normalized.append(normalize3(v, m, var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr = np.asarray(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test= train_test_split(descr,test_size=0.3, shuffle = True, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape for convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[2],1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[2],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builiding the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential([keras.Input(shape=( 1, 128))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layers\n",
    "encoder.add(Dense(128, activation = \"tanh\"))\n",
    "encoder.add(Dropout(0.1))\n",
    "encoder.add(Dense(64, activation = \"tanh\"))\n",
    "encoder.add(Dropout(0.1))\n",
    "encoder.add(Dense(32, activation = \"tanh\"))\n",
    "encoder.add(Dropout(0.1))\n",
    "encoder.add(Dense(16, activation = \"tanh\"))\n",
    "encoder.add(Dropout(0.1))\n",
    "encoder.add(Dense(8, activation = \"tanh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Sequential([keras.Input(shape=( 128, 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.add(Conv1D(8,3,activation='tanh', padding = \"same\"))\n",
    "encoder.add(MaxPooling1D(2))\n",
    "encoder.add(Conv1D(4,3,activation='tanh', padding = \"same\"))\n",
    "encoder.add(MaxPooling1D(2))\n",
    "encoder.add(AveragePooling1D())\n",
    "encoder.add(Flatten())\n",
    "encoder.add(Dense(2, activation = \"tanh\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 128, 8)            32        \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 64, 8)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 64, 4)             100       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 4)             0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 16, 4)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 132\n",
      "Trainable params: 132\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Sequential([keras.Input(shape=(1,8))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.add(Dense(16, activation = \"tanh\"))\n",
    "decoder.add(Dropout(0.1))\n",
    "decoder.add(Dense(32, activation = \"tanh\"))\n",
    "decoder.add(Dropout(0.1))\n",
    "decoder.add(Dense(64, activation = \"tanh\"))\n",
    "decoder.add(Dropout(0.1))\n",
    "decoder.add(Dense(128, activation = \"tanh\"))\n",
    "decoder.add(Dense(128, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Sequential([keras.Input(shape=(64))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.add(Dense(64, activation = \"tanh\"))\n",
    "decoder.add(Reshape((16,4), input_shape = (64,)))\n",
    "decoder.add(Conv1D(4,1, strides =1, activation='tanh', padding = \"same\"))\n",
    "decoder.add(UpSampling1D(2))\n",
    "decoder.add(Conv1D(8,1, strides =1, activation='tanh', padding = \"same\"))\n",
    "decoder.add(UpSampling1D(2))\n",
    "decoder.add(UpSampling1D(2))\n",
    "decoder.add(Conv1D(1,1, strides =1, activation='sigmoid', padding = \"same\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 16, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 4)             20        \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 32, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 32, 8)             40        \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 64, 8)             0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 128, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 128, 1)            9         \n",
      "=================================================================\n",
      "Total params: 69\n",
      "Trainable params: 69\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Sequential([keras.Input(shape=(128,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.add(encoder)\n",
    "autoencoder.add(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 64)                132       \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 128, 1)            69        \n",
      "=================================================================\n",
      "Total params: 201\n",
      "Trainable params: 201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 100\n",
    "bs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [EarlyStopping(monitor='accuracy', patience=8, verbose=1, mode='auto')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs= nb_epochs,\n",
    "                batch_size= bs)\n",
    "#validation_data=(X_test, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "#import run_logdir from tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"autoencoder_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1/72 [..............................] - ETA: 0s - loss: 0.1959 - mae: 0.4259WARNING:tensorflow:From /home/belen_alastruey/PYTHON/envs/pythonIntro/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0051s vs `on_train_batch_end` time: 0.0156s). Check your callbacks.\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.1229 - mae: 0.3279 - val_loss: 0.0605 - val_mae: 0.2214\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0464 - mae: 0.1754 - val_loss: 0.0412 - val_mae: 0.1508\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0404 - mae: 0.1432 - val_loss: 0.0400 - val_mae: 0.1381\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0399 - mae: 0.1358 - val_loss: 0.0398 - val_mae: 0.1343\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0397 - mae: 0.1337 - val_loss: 0.0397 - val_mae: 0.1332\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.0396 - mae: 0.1331 - val_loss: 0.0396 - val_mae: 0.1330\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0395 - mae: 0.1330 - val_loss: 0.0395 - val_mae: 0.1331\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0395 - mae: 0.1333 - val_loss: 0.0394 - val_mae: 0.1331\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0394 - mae: 0.1332 - val_loss: 0.0394 - val_mae: 0.1334\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0393 - mae: 0.1336 - val_loss: 0.0393 - val_mae: 0.1334\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0393 - mae: 0.1336 - val_loss: 0.0392 - val_mae: 0.1338\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0392 - mae: 0.1340 - val_loss: 0.0391 - val_mae: 0.1340\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0390 - mae: 0.1344 - val_loss: 0.0388 - val_mae: 0.1349\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0385 - mae: 0.1359 - val_loss: 0.0380 - val_mae: 0.1370\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0372 - mae: 0.1371 - val_loss: 0.0362 - val_mae: 0.1356\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0356 - mae: 0.1335 - val_loss: 0.0352 - val_mae: 0.1310\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.1297 - val_loss: 0.0348 - val_mae: 0.1281\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0348 - mae: 0.1278 - val_loss: 0.0347 - val_mae: 0.1268\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.1269 - val_loss: 0.0346 - val_mae: 0.1259\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0346 - mae: 0.1262 - val_loss: 0.0346 - val_mae: 0.1255\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0346 - mae: 0.1258 - val_loss: 0.0346 - val_mae: 0.1253\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0346 - mae: 0.1255 - val_loss: 0.0345 - val_mae: 0.1248\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0346 - mae: 0.1252 - val_loss: 0.0345 - val_mae: 0.1246\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1250 - val_loss: 0.0345 - val_mae: 0.1244\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1249 - val_loss: 0.0345 - val_mae: 0.1243\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1248 - val_loss: 0.0345 - val_mae: 0.1243\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1246 - val_loss: 0.0344 - val_mae: 0.1243\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1245 - val_loss: 0.0344 - val_mae: 0.1243\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1244 - val_loss: 0.0344 - val_mae: 0.1242\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0345 - mae: 0.1244 - val_loss: 0.0344 - val_mae: 0.1241\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1243 - val_loss: 0.0344 - val_mae: 0.1237\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1242 - val_loss: 0.0344 - val_mae: 0.1238\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1241 - val_loss: 0.0344 - val_mae: 0.1237\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1241 - val_loss: 0.0344 - val_mae: 0.1237\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1240 - val_loss: 0.0344 - val_mae: 0.1236\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1240 - val_loss: 0.0344 - val_mae: 0.1235\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1239 - val_loss: 0.0344 - val_mae: 0.1235\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1238 - val_loss: 0.0343 - val_mae: 0.1232\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1238 - val_loss: 0.0343 - val_mae: 0.1233\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1237 - val_loss: 0.0343 - val_mae: 0.1231\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1237 - val_loss: 0.0343 - val_mae: 0.1235\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1237 - val_loss: 0.0343 - val_mae: 0.1230\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1236 - val_loss: 0.0343 - val_mae: 0.1233\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1236 - val_loss: 0.0343 - val_mae: 0.1232\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1235 - val_loss: 0.0343 - val_mae: 0.1230\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1235 - val_loss: 0.0343 - val_mae: 0.1232\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.1235 - val_loss: 0.0343 - val_mae: 0.1229\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1234 - val_loss: 0.0343 - val_mae: 0.1229\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1234 - val_loss: 0.0343 - val_mae: 0.1228\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1233 - val_loss: 0.0343 - val_mae: 0.1229\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1233 - val_loss: 0.0343 - val_mae: 0.1228\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1233 - val_loss: 0.0343 - val_mae: 0.1230\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1233 - val_loss: 0.0343 - val_mae: 0.1227\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1233 - val_loss: 0.0343 - val_mae: 0.1228\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1232 - val_loss: 0.0343 - val_mae: 0.1227\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1232 - val_loss: 0.0343 - val_mae: 0.1230\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1232 - val_loss: 0.0343 - val_mae: 0.1229\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1231 - val_loss: 0.0343 - val_mae: 0.1225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1231 - val_loss: 0.0343 - val_mae: 0.1231\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1231 - val_loss: 0.0343 - val_mae: 0.1228\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1231 - val_loss: 0.0343 - val_mae: 0.1230\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1231 - val_loss: 0.0343 - val_mae: 0.1225\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1230 - val_loss: 0.0343 - val_mae: 0.1224\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1230 - val_loss: 0.0343 - val_mae: 0.1232\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1230 - val_loss: 0.0343 - val_mae: 0.1224\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1230 - val_loss: 0.0343 - val_mae: 0.1225\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1230 - val_loss: 0.0342 - val_mae: 0.1225\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1230 - val_loss: 0.0342 - val_mae: 0.1223\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1226\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1225\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1225\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1225\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1221\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1229 - val_loss: 0.0342 - val_mae: 0.1221\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1224\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1227\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1224\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1226\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1224\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1221\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1227\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1228 - val_loss: 0.0342 - val_mae: 0.1227\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1219\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1220\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1218\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1221\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1224\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1220\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1224\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1223\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1222\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1217\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.1227 - val_loss: 0.0342 - val_mae: 0.1224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81f84f11c0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tensorboard, extra parameter: tensorboard --logdir=\"my_logs/\"\n",
    "autoencoder.fit(X_train, X_train, epochs=nb_epochs, batch_size=bs, validation_data=(X_test, X_test),callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 2ms/step - loss: 0.0342 - mae: 0.1224\n",
      "[0.03420661762356758, 0.12235258519649506]\n"
     ]
    }
   ],
   "source": [
    "score = autoencoder.evaluate(X_test, X_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('convolucional3_eam0122.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
